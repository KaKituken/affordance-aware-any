fg-train-transform: &fg-train-transform-config
  - Resize:
      fix_size:
      - 224
      - 224
  - RandomResize:
      factor:
      - 0.8
      - 1.2
      isotropic: true
      prob: 0.4
  - RandomHorizonFlip:
      prob: 0.5
  - RandomRotation:
      angle: 30
      prob: 0.4
  - RandomResize:
      factor:
      - 0.8
      - 1.2
      isotropic: false
      prob: 0.2
  - RandomCutout:
      cut_factor:
      - 0.1
      - 0.5
      prob: 0.2
  - CropOrPad:
      fix_size:
      - 224
      - 224
  - Brightness:
      brightness: 0.5
      prob: 0.2
  - Contrast:
      contrast: 0.5
      prob: 0.2
  - Saturation:
      saturation: 0.5
      prob: 0.2
  - GaussianBlur:
      kernel_size: 3
      prob: 0.1
  - DinoImage:
      path: facebook/dinov2-large
      return_tensors: pt
      padding: False
  - AddNoise:
      noise: 0.1
      prob: 0.1

fg-val-transform: &fg-val-transform-config
  - Resize:
      fix_size:
      - 224
      - 224
  - DinoImage:
      path: facebook/dinov2-large
      return_tensors: pt
      padding: False

bg-train-transform: &bg-train-transform-config
  - ResizeWithAnn:
      size: 256
  - CropWithAnn:
      size: 256
  - CropToMultipleWithAnn:
      factor: 8
  - JitterPoint:
      factor: 0.1
  - EnlargeMask:
      factor: 
        - 1.2
        - 1.5
  - CenterEnlargeBbox:
      factor:
        - 1.2
        - 1.5
  - FeatherMask:
      sigma: 9
  - ToTensorWithAnn: {}
  - RescaleWithAnn:
      scale_range:
      - -1
      - 1

bg-val-transform: &bg-val-transform-config
  - ResizeWithAnn:
      size: 256
  - CropWithAnn:
      size: 256
  - CropToMultipleWithAnn:
      factor: 8
  - ToTensorWithAnn: {}
  - RescaleWithAnn:
      scale_range:
      - -1
      - 1

model:
  base_learning_rate: 1.25e-4
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    ckpt_path: /path/to/sd-v1-5-inpainting.ckpt # TODO: point to the checkpoint
    ignore_keys:
      - "load_maximally"
      - "cond_stage_model"
    linear_start: 0.00085
    linear_end: 0.0120
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: groundtruth
    cond_stage_key:
      # left
      concat:
        - key: background
          encoder: first
        # sample from three different types of prompts (how to deal with null prompt?)
        - key: mask
          encoder: rescale
        - key: org_mask
          encoder: rescale
        - key: bbox
          encoder: coor2area
        - key: point
          encoder: heatmap
        - key: blank
          encoder: blank
      # condition
      crossattn:
        - key: foreground
          encoder: cond
    conditioning_key: hybrid  # use both concat & crossattn
    # conditioning_key: concat  # debug
    sample_cond:
      - key: point
        p: 1
      - key: bbox
        p: 0
      - key: mask
        p: 0
    image_size: 32
    channels: 5   # !!! modify here to match Dual Diffusion. 5 learnable channels
    cond_stage_trainable: false   # Note: different from the one we trained before
    monitor: val/loss_simple_ema
    loss_type: l2
    scale_factor: 0.18215
    use_ema: True         # stable diffusion inference sets it to False
    dual_diffusion: True  # use dual diffusion process to predict position
    change_order: True    # !!! always change order
    
    scheduler_config: # 10000 warmup steps
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [ 5000 ]
        cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
        f_start: [ 1.e-6 ]
        f_max: [ 1. ]
        f_min: [ 1. ]

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32
        in_channels: 8    # 8 channels, separately
        pos_in_channels: 2
        out_channels: 4   # z
        pos_out_channels: 1
        branch_block_num: 1
        input_branch_block_num: 1
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        use_spatial_transformer: True
        transformer_depth: 1
        raw_context_dim: 1024
        context_dim: 768
        use_checkpoint: False
        legacy: False
        compress: True
        compress_method: cross-attn
    
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenHFDinoV2ImageEmbedder
    
    rescale_stage_config:
      target: ldm.modules.encoders.modules.SpatialRescaler
      params:
        n_stages: 3 # 256 -> 32; 128 -> 16
        method: 'bilinear'
        augment: false

    coor2area_stage_config:
      target: ldm.modules.encoders.modules.Coor2Area
      params:
        target_size: [32, 32]
        augment: false
    
    heatmap_stage_config:
      target: ldm.modules.encoders.modules.GuassHeatmapper
      params: 
        target_size: [32, 32]
        sigma: 2.6
        augment: false
    
    blank_stage_config:
      target: ldm.modules.encoders.modules.BlankGenerator
      params:
        target_size: [32, 32]

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 32
    num_workers: 14
    wrap: false
    train:  &train-data-config
      target: ldm.data.fbp.ImageVideoWrapper
      params:
        image_cfg:
          fbp_path: 'data/SAM-FB/'# TODO: change to the path of the dataset
          gt_path: 'data/SAM-FB/' # TODO: change to the path of the dataset
          num_folder: 900
          transform:
            foreground:
              <<: *fg-train-transform-config
            background_with_groundtruth:
              <<: *bg-train-transform-config
        video_cfg:
          fbp_path: 'data/SAM-FB-video' # TODO: change to the path of the dataset
          gt_path: 'data/SAM-FB-video'  # TODO: change to the path of the dataset
          num_folder: 900
          transform:
            foreground:
              <<: *fg-train-transform-config
            background_with_groundtruth:
              <<: *bg-train-transform-config
        ratio: 0.5  # TODO: change to the ratio of using image and video data
    validation: &val-data-config
      target: ldm.data.fbp.ImageVideoWrapper
      params:
        image_cfg:
          fbp_path: 'data/SAM-FB-Test'  # TODO: change to the path of the dataset
          gt_path: 'data/SAM-FB-Test'   # TODO: change to the path of the dataset
          num_folder: 100
          mode: val
          transform:
            foreground:
              <<: *fg-val-transform-config
            background_with_groundtruth:
              <<: *bg-val-transform-config
        video_cfg:
          fbp_path: 'data/SAM-FB-Video-Test'  # TODO: change to the path of the dataset
          gt_path: 'data/SAM-FB-Video-Test'   # TODO: change to the path of the dataset
          num_folder: 100
          mode: val
          transform:
            foreground:
              <<: *fg-val-transform-config
            background_with_groundtruth:
              <<: *bg-val-transform-config
        ratio: 0.5
lightning:
  callbacks:
    metrics_over_trainsteps_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        every_n_train_steps: 2500

    # already modified
    image_logger:
      target: main.ImageLogger
      params:
        batch_frequency: 10001
        # batch_frequency: 1
        max_images: 8
        increase_log_steps: false
    
    # already modified
    fid_test:
      target: main.FIDCallback
      params:
        batch_size: 8
        batch_freq: 200
        num_workers: 2
        ddim_steps: 50
        max_batch: 8
        cfg_scales:
          - 4.0
        calc_fid_score: true
        gt_dir: 'path/to/save/validation/result'  # TODO: change to the path to save the result
        pd_dir: 'path/to/save/validation/result'  # TODO: change to the path to save the result
        dataset_cfg:
          <<: *val-data-config

  trainer:
    strategy: ddp_find_unused_parameters_false
    max_epochs: 20
    benchmark: True
    accumulate_grad_batches: 1
    limit_val_batches: 0.0